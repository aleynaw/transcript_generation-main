{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f7438eb-8536-4164-9a49-21560abd887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('movie_scripts.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c92164-6c31-4ca5-91b9-007507d4def4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          episode  episode_order  \\\n",
      "0          57264              9   \n",
      "1          57264             10   \n",
      "2          57264             11   \n",
      "3          57264             12   \n",
      "4          57264             13   \n",
      "...          ...            ...   \n",
      "3199853    67560             11   \n",
      "3199854    67560             12   \n",
      "3199855    67560             13   \n",
      "3199856    67560             14   \n",
      "3199857    67560             15   \n",
      "\n",
      "                                                   speaker  \\\n",
      "0        Ms. LOREN MOONEY (Editor-in-Chief, Bicycling M...   \n",
      "1        Ms. LOREN MOONEY (Editor-in-Chief, Bicycling M...   \n",
      "2                                         NEAL CONAN, host   \n",
      "3        Ms. LOREN MOONEY (Editor-in-Chief, Bicycling M...   \n",
      "4                                         NEAL CONAN, host   \n",
      "...                                                    ...   \n",
      "3199853                                        _NO_SPEAKER   \n",
      "3199854                            MARY LOUISE KELLY, HOST   \n",
      "3199855                            MARY LOUISE KELLY, HOST   \n",
      "3199856                                  ARI SHAPIRO, HOST   \n",
      "3199857                            MARY LOUISE KELLY, HOST   \n",
      "\n",
      "                                                 utterance  \n",
      "0        It's a 2,200-mile race. To give some sense of ...  \n",
      "1        So for a top competitor like Lance to try to m...  \n",
      "2        So in every team, presumably there's one star,...  \n",
      "3        That's right. Each team has nine riders. And w...  \n",
      "4        So slipstream, this is like drafting in car ra...  \n",
      "...                                                    ...  \n",
      "3199853                  (SOUNDBITE OF ARCHIVED RECORDING)  \n",
      "3199854  UNIDENTIFIED MAN #2: Terrifying moments inside...  \n",
      "3199855  That girl brought a gun to school, and it acci...  \n",
      "3199856  On January 31 - gunshots outside a high school...  \n",
      "3199857  January 26, shots were fired in a high school ...  \n",
      "\n",
      "[3199858 rows x 4 columns]>\n",
      "Average Distinct-1 over 3199858 scripts: 0.8907\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('utterances.csv')\n",
    "print(df.head)\n",
    "\n",
    "# Apply to each row\n",
    "df['distinct_1'] = df['utterance'].apply(distinct_1)\n",
    "\n",
    "# Compute the average Distinct-1 score across all scripts\n",
    "mean_distinct_1 = df['distinct_1'].mean()\n",
    "\n",
    "print(f\"Average Distinct-1 over {len(df)} scripts: {mean_distinct_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aa8064f-4002-430c-a98e-8b54a5edee54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   episode  distinct_1_score\n",
      "0        1          0.521545\n",
      "1        2          0.484067\n",
      "2        3          0.544326\n",
      "3        4          0.506652\n",
      "4        5          0.407384\n",
      "Average episode‐level Distinct-1: 0.5024\n"
     ]
    }
   ],
   "source": [
    "def distinct_1_episode(texts: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Compute Distinct-1 over all utterances in an episode:\n",
    "      (# unique unigrams across the episode) \n",
    "      / (total # unigrams across the episode).\n",
    "    \"\"\"\n",
    "    # concatenate all utterances into one big string\n",
    "    all_tokens = (\n",
    "        \" \".join(texts.dropna().astype(str))\n",
    "        .lower()\n",
    "        .split()\n",
    "    )\n",
    "    if not all_tokens:\n",
    "        return 0.0\n",
    "    return len(set(all_tokens)) / len(all_tokens)\n",
    "\n",
    "# assume your episode identifier column is named \"episode\"\n",
    "episode_scores = (\n",
    "    df\n",
    "    .groupby(\"episode\")[\"utterance\"]\n",
    "    .apply(distinct_1_episode)\n",
    "    .reset_index(name=\"distinct_1_score\")\n",
    ")\n",
    "\n",
    "# peek at per‐episode scores\n",
    "print(episode_scores.head())\n",
    "\n",
    "# compute the average across episodes\n",
    "mean_ep_score = episode_scores[\"distinct_1_score\"].mean()\n",
    "print(f\"Average episode‐level Distinct-1: {mean_ep_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c73eb781-97a4-4ec0-a977-af3a9f01aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw unique count: 104920\n",
      "Some raw values: [57264 58225 75004 74884 63416 68175 67560 68670 70039 75394 58131 68325\n",
      " 80651 60226 72046 80598 68420 79679 60873 65791]\n",
      "Cleaned unique count: 104920\n",
      "Some cleaned values: ['57264' '58225' '75004' '74884' '63416' '68175' '67560' '68670' '70039'\n",
      " '75394' '58131' '68325' '80651' '60226' '72046' '80598' '68420' '79679'\n",
      " '60873' '65791']\n",
      "episode_clean\n",
      "19633    569\n",
      "35108    552\n",
      "57481    539\n",
      "73336    492\n",
      "66523    480\n",
      "65241    461\n",
      "60824    445\n",
      "52192    441\n",
      "81521    431\n",
      "82700    426\n",
      "Name: count, dtype: int64\n",
      "       episode_clean  distinct_1_score\n",
      "0                  1          0.521545\n",
      "1                 10          0.436911\n",
      "2                100          0.511384\n",
      "3              10000          0.441989\n",
      "4             100000          0.558205\n",
      "...              ...               ...\n",
      "104915         99995          0.660274\n",
      "104916         99996          0.538937\n",
      "104917         99997          0.524800\n",
      "104918         99998          0.409127\n",
      "104919         99999          0.471366\n",
      "\n",
      "[104920 rows x 2 columns]\n",
      "Average over episodes: 0.5024092587964564\n"
     ]
    }
   ],
   "source": [
    "# 1) Check how many unique episode IDs you really have:\n",
    "print(\"Raw unique count:\", df['episode'].nunique())\n",
    "print(\"Some raw values:\", df['episode'].unique()[:20])\n",
    "\n",
    "# 2) Clean up whitespace/case (very common culprit):\n",
    "df['episode_clean'] = (\n",
    "    df['episode']\n",
    "      .astype(str)        # make sure everything is string\n",
    "      .str.strip()         # remove leading/trailing spaces\n",
    "      .str.lower()         # unify case, if e.g. “Ep1” vs “ep1”\n",
    ")\n",
    "\n",
    "# 3) Re-inspect:\n",
    "print(\"Cleaned unique count:\", df['episode_clean'].nunique())\n",
    "print(\"Some cleaned values:\", df['episode_clean'].unique()[:20])\n",
    "print(df['episode_clean'].value_counts().head(10))\n",
    "\n",
    "# 4) Now group on the cleaned IDs:\n",
    "def distinct_1_episode(texts):\n",
    "    tokens = \" \".join(texts.dropna().astype(str)).lower().split()\n",
    "    return len(set(tokens)) / len(tokens) if tokens else 0.0\n",
    "\n",
    "episode_scores = (\n",
    "    df\n",
    "      .groupby('episode_clean')['utterance']\n",
    "      .apply(distinct_1_episode)\n",
    "      .reset_index(name='distinct_1_score')\n",
    ")\n",
    "\n",
    "print(episode_scores)\n",
    "print(\"Average over episodes:\", episode_scores['distinct_1_score'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56422363-441c-4f0d-9f2b-6d2869b5f7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD of episode‐level Distinct-1: 0.1052\n"
     ]
    }
   ],
   "source": [
    "std_ep  = episode_scores['distinct_1_score'].std()\n",
    "print(f\"STD of episode‐level Distinct-1: {std_ep:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (patient_creator)",
   "language": "python",
   "name": "patient_creator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
