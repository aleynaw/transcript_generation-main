{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ytcao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ytcao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import time\n",
    "from dotenv.main import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import ast\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_csv_path = \"./analysis.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_csv_path = \"../transcript_generation/tracking.csv\"\n",
    "tracking_df = pd.read_csv(tracking_csv_path, sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../transcript_generation/transcripts/gpt4_v2/DM/DM_20240428-221529_Interview.json', '../transcript_generation/transcripts/gpt4_v2/DM/DM_20240428-223120_Interview.json', '../transcript_generation/transcripts/gpt4_v2/DM/DM_20240428-220138_Interview.json', '../transcript_generation/transcripts/gpt4_v2/DM/DM_20240428-222702_Interview.json', '../transcript_generation/transcripts/gpt4_v2/DM/DM_20240428-215539_Interview.json', '../transcript_generation/transcripts/gpt4_v2/DM/DM_20240428-221850_Interview.json', '../transcript_generation/transcripts/gpt4_v2/DM/DM_20240428-222418_Interview.json', '../transcript_generation/transcripts/gpt4_v2/DM/DM_20240428-221238_Interview.json', '../transcript_generation/transcripts/gpt4_v2/DM/DM_20240428-220641_Interview.json', '../transcript_generation/transcripts/gpt4_v2/DM/DM_20240428-220920_Interview.json']\n",
      "['../transcript_generation/transcripts/gpt4_v2/SM/SM_20240428-220920_Interview.json', '../transcript_generation/transcripts/gpt4_v2/SM/SM_20240428-222418_Interview.json', '../transcript_generation/transcripts/gpt4_v2/SM/SM_20240428-221850_Interview.json', '../transcript_generation/transcripts/gpt4_v2/SM/SM_20240428-221238_Interview.json', '../transcript_generation/transcripts/gpt4_v2/SM/SM_20240428-220641_Interview.json', '../transcript_generation/transcripts/gpt4_v2/SM/SM_20240428-222702_Interview.json', '../transcript_generation/transcripts/gpt4_v2/SM/SM_20240428-215539_Interview.json', '../transcript_generation/transcripts/gpt4_v2/SM/SM_20240428-223120_Interview.json', '../transcript_generation/transcripts/gpt4_v2/SM/SM_20240428-220138_Interview.json', '../transcript_generation/transcripts/gpt4_v2/SM/SM_20240428-221529_Interview.json']\n"
     ]
    }
   ],
   "source": [
    "#load in the relevant files from a directory\n",
    "dm_transcript_dir= Path(\"../transcript_generation/transcripts/gpt4_v2/DM\")\n",
    "sm_transcript_dir= Path(\"../transcript_generation/transcripts/gpt4_v2/SM\")\n",
    "dm_transcript_list=glob.glob(str(dm_transcript_dir)+\"/*.json\")\n",
    "sm_transcript_list=glob.glob(str(sm_transcript_dir)+\"/*.json\")\n",
    "print(dm_transcript_list)\n",
    "print(sm_transcript_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracking_df[(tracking_df[\"interview_path\"]==sm_transcript_list[1].split('/')[-1])].dropna(axis=1)\n",
    "# tracking_df[(tracking_df[\"interview_path\"])==dm_transcript_list[0].split('/')[-1]][\"patient_str\"].to_list()\n",
    "# print(tracking_df[(tracking_df[\"interview_path\"])==sm_transcript_list[1].split('/')[-1]][\"patient_temp\"].to_list()[0] == np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(word_list):\n",
    "    '''word_list is a list of individual words.'''\n",
    "    return len(set(word_list))/len(word_list)\n",
    "\n",
    "def create_word_list(message_list):\n",
    "    word_str = \" \".join(message_list)\n",
    "    word_str=word_str.lower()\n",
    "    # print(asst_str)\n",
    "    word_list = tokenize.word_tokenize(word_str)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convo_stats(transcript_path):\n",
    "    out_dict = {}\n",
    "    out_dict[\"filename\"] = transcript_path.split('/')[-1]\n",
    "    \n",
    "    if out_dict[\"filename\"][:2] == \"DM\":\n",
    "        out_dict[\"is_double_model\"] = True\n",
    "    else:\n",
    "        out_dict[\"is_double_model\"] = False\n",
    "\n",
    "    ### match tracking info\n",
    "    tracked_info = tracking_df[(tracking_df[\"interview_path\"])==transcript_path.split('/')[-1]].dropna(axis=1)\n",
    "    out_dict[\"patient_str\"] = tracked_info[\"patient_str\"].to_list()[0]\n",
    "    if \"rambling\" in out_dict[\"patient_str\"]:\n",
    "        out_dict[\"is_rambling_prompt\"]=True\n",
    "    else:\n",
    "        out_dict[\"is_rambling_prompt\"]=False\n",
    "    out_dict[\"total_cost\"] = tracked_info[\"total_cost\"].to_list()[0]\n",
    "    out_dict[\"time\"] = tracked_info[\"time\"].to_list()[0]\n",
    "    \n",
    "    if \"edge_case\" in tracked_info:\n",
    "        out_dict[\"edge_case\"] = tracked_info[\"edge_case\"].to_list()[0]\n",
    "    else:\n",
    "        out_dict[\"edge_case\"] = \"\"\n",
    "\n",
    "    if \"patient_temp\" in tracked_info:\n",
    "        out_dict[\"user_temp\"] = tracked_info[\"patient_temp\"].to_list()[0]\n",
    "        out_dict[\"asst_temp\"] = tracked_info[\"assistant_temp\"].to_list()[0]\n",
    "        out_dict[\"temp\"] = \"\"\n",
    "    else:\n",
    "        out_dict[\"user_temp\"] = \"\"\n",
    "        out_dict[\"asst_temp\"] = \"\"\n",
    "        out_dict[\"temp\"] = tracked_info[\"temp\"].to_list()[0]\n",
    "    \n",
    "    ### load transcript info\n",
    "    with open(transcript_path) as f:\n",
    "        convo = json.load(f)\n",
    "        f.close()\n",
    "    # print(len(convo))\n",
    "    # print((len(convo)-1)/2)\n",
    "    out_dict[\"convo_length\"] = len(convo)-1\n",
    "    out_dict[\"convo_rounds\"] = (len(convo)-1)/2 #number of user-assistant messages; first message is automated\n",
    "\n",
    "    ### load assistant messages\n",
    "    asst_msgs = [message[\"content\"] for message in convo if message[\"role\"]==\"assistant\"][1:] #ignore the first message\n",
    "    # print(len(asst_msgs))\n",
    "    out_dict[\"asst_utt\"] = len(asst_msgs)\n",
    "    out_dict[\"'asst_messages'\"] = \"|\".join(asst_msgs)\n",
    "\n",
    "    ### get distinct-1\n",
    "    asst_text = create_word_list(asst_msgs)\n",
    "    out_dict[\"asst_distinct1\"] = lexical_diversity(asst_text)\n",
    "\n",
    "    # look at every single assistant message\n",
    "    asst_msg_length = []\n",
    "    # asst_sentences = [] #process the assistant sentences in gpt_analysis\n",
    "    for msg in asst_msgs:\n",
    "        asst_msg_length.append(len(msg))\n",
    "        # asst_sentences.append(tokenize.sent_tokenize(msg))\n",
    "    # print(asst_msg_length)\n",
    "    # print(np.average(asst_msg_length))\n",
    "    # out_dict[\"'asst_sentences'\"] = \"|\".join([item for sublist in asst_sentences for item in sublist])\n",
    "    \n",
    "    out_dict[\"'asst_msg_len'\"] = \"|\".join([str(length) for length in asst_msg_length])\n",
    "    out_dict[\"asst_avg_msg_len\"] = np.average(asst_msg_length)\n",
    "\n",
    "    ### load user messages\n",
    "    user_msgs = [message[\"content\"] for message in convo if message[\"role\"]==\"user\"]\n",
    "    # print(len(user_msgs))\n",
    "    out_dict[\"user_utt\"] = len(user_msgs)\n",
    "    out_dict[\"'user_messages'\"] = \"|\".join(user_msgs)\n",
    "    \n",
    "    ### get distinct-1\n",
    "    user_text = create_word_list(user_msgs)\n",
    "    out_dict[\"user_distinct1\"] = lexical_diversity(user_text)\n",
    "\n",
    "    # look at every single user message\n",
    "    user_msg_length = []\n",
    "    # user_sentences = []\n",
    "    for msg in user_msgs:\n",
    "        user_msg_length.append(len(msg))\n",
    "        # user_sentences.append(tokenize.sent_tokenize(msg))\n",
    "    # print(user_msg_length)\n",
    "    # print(np.average(user_msg_length))\n",
    "    # out_dict[\"'user_sentences'\"] = \"|\".join([item for sublist in user_sentences for item in sublist])\n",
    "    out_dict[\"'user_msg_len'\"] = \"|\".join([str(length) for length in user_msg_length])\n",
    "    out_dict[\"user_avg_msg_len\"] = np.average(user_msg_length)\n",
    "    \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get convo statistics/analysis.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict_list = []\n",
    "for transcript in dm_transcript_list:\n",
    "    print(transcript)\n",
    "    out_dict = get_convo_stats(transcript)\n",
    "    out_dict_list.append(out_dict)\n",
    "for transcript in sm_transcript_list:\n",
    "    print(transcript)\n",
    "    out_dict = get_convo_stats(transcript)\n",
    "    out_dict_list.append(out_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_out_dict = {}\n",
    "for d in out_dict_list:\n",
    "    for k, v in d.items():\n",
    "        super_out_dict.setdefault(k,[]).append(v)\n",
    "\n",
    "out_df = pd.DataFrame(super_out_dict)\n",
    "out_df.to_csv(out_csv_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at distinct-1 measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transcript = dm_transcript_list[0]\n",
    "with open(test_transcript) as f:\n",
    "    convo = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "asst_msgs = [message[\"content\"] for message in convo if message[\"role\"]==\"assistant\"][1:]\n",
    "user_msgs = [message[\"content\"] for message in convo if message[\"role\"]==\"user\"]\n",
    "\n",
    "#turn message list into one string\n",
    "asst_wordlist = create_word_list(user_msgs)\n",
    "user_wordlist = create_word_list(user_msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asst_wordlist_nostops =  set(asst_wordlist) - set(stops)\n",
    "\n",
    "\n",
    "fd_asst = nltk.FreqDist(asst_wordlist_nostops)\n",
    "fd_asst1 = nltk.FreqDist(asst_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_asst.most_common(10)\n",
    "# fd_asst1.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graphing\n",
    "read the analysis csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.read_csv(out_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df[analysis_df[\"is_double_model\"]==True][analysis_df[\"is_rambling_prompt\"]==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_col = {\n",
    "    \"DM-rambling\": \"blue\",\n",
    "    \"DM-brief\":\"green\",\n",
    "    \"SM-rambling\": \"yellow\",\n",
    "    \"SM-brief\":\"red\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = pd.concat(\n",
    "    [analysis_df[analysis_df[\"is_double_model\"]==True][analysis_df[\"is_rambling_prompt\"]==True].assign(dataset=\"DM-rambling\", bar_col=bar_col[\"DM-rambling\"]), \n",
    "    analysis_df[analysis_df[\"is_double_model\"]==True][analysis_df[\"is_rambling_prompt\"]==False].assign(dataset=\"DM-brief\", bar_col=bar_col[\"DM-brief\"]),\n",
    "    analysis_df[analysis_df[\"is_double_model\"]==False][analysis_df[\"is_rambling_prompt\"]==True].assign(dataset=\"SM-rambling\", bar_col=bar_col[\"SM-rambling\"]),\n",
    "    analysis_df[analysis_df[\"is_double_model\"]==False][analysis_df[\"is_rambling_prompt\"]==False].assign(dataset=\"SM-brief\", bar_col=bar_col[\"SM-brief\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dfm = pd.melt(labeled_df, \n",
    "                      id_vars=[\"dataset\",\"bar_col\"], #data to keep as the identifier\n",
    "                      value_vars=[\"asst_avg_msg_len\", \"user_avg_msg_len\"], #columns to \"unpivot\" or \"melt\" together\n",
    "                      var_name=\"speaker\", #name of new unpivoted col\n",
    "                      value_name=\"avg_msg_len\") #name of the new col value\n",
    "labeled_dfm = labeled_dfm.replace(\"asst_avg_msg_len\", \"assistant\")\n",
    "labeled_dfm = labeled_dfm.replace(\"user_avg_msg_len\", \"patient\")\n",
    "labeled_dfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pvalue_to_asterisks(pvalue, bf_correction):\n",
    "    if pvalue <= 0.0001/bf_correction:\n",
    "        return \"****\"\n",
    "    elif pvalue <= 0.001/bf_correction:\n",
    "        return \"***\"\n",
    "    elif pvalue <= 0.01/bf_correction:\n",
    "        return \"**\"\n",
    "    elif pvalue <= 0.05/bf_correction:\n",
    "        return \"*\"\n",
    "    return \"ns\"\n",
    "\n",
    "def get_dataset_comparison_pvalues(dfm, col, bf_correction = False):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfm : pandas DataFrame \n",
    "        with columns \"dataset\" and \"speaker\" as either assistant/patient. values in the \"dataset\" column will be paired.\n",
    "    col : str\n",
    "        name of column in dfm that pvalue should be calculated from\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    list\n",
    "    '''\n",
    "    x_values = dfm[\"dataset\"].unique()\n",
    "    pvalues_list = []\n",
    "    done = []\n",
    "    for x in x_values:\n",
    "        for x1 in x_values:\n",
    "            if x != x1 and x1 not in done:\n",
    "                asst_stat, asst_pvalue = scipy.stats.ttest_ind(\n",
    "                    dfm[(dfm[\"dataset\"] == x) & (dfm[\"speaker\"] == \"assistant\")][col],\n",
    "                    dfm[(dfm[\"dataset\"] == x1) & (dfm[\"speaker\"] == \"assistant\")][col]\n",
    "                )\n",
    "                user_stat, user_pvalue = scipy.stats.ttest_ind(\n",
    "                    dfm[(dfm[\"dataset\"] == x) & (dfm[\"speaker\"] == \"patient\")][col],\n",
    "                    dfm[(dfm[\"dataset\"] == x1) & (dfm[\"speaker\"] == \"patient\")][col]\n",
    "                )\n",
    "                pvalues_list.append(((x, x1), \n",
    "                                        {\"assistant\":(asst_pvalue), \n",
    "                                        \"patient\":(user_pvalue)}))\n",
    "                \n",
    "            done.append(x)\n",
    "    if bf_correction:\n",
    "        corr_val = len(done)\n",
    "        pvalues_list = [((x, x1), \n",
    "                         {\"assistant\": (pvalue_dict[\"assistant\"], \n",
    "                                        convert_pvalue_to_asterisks(pvalue_dict[\"assistant\"], corr_val)),\n",
    "                          \"patient\":  (pvalue_dict[\"patient\"], \n",
    "                                       convert_pvalue_to_asterisks(pvalue_dict[\"patient\"], corr_val))\n",
    "                          }) for ((x, x1), pvalue_dict) in pvalues_list]\n",
    "    else:\n",
    "        pvalues_list = [((x, x1), \n",
    "                         {\"assistant\": (pvalue_dict[\"assistant\"], \n",
    "                                        convert_pvalue_to_asterisks(pvalue_dict[\"assistant\"], 1)),\n",
    "                          \"patient\":  (pvalue_dict[\"patient\"], \n",
    "                                       convert_pvalue_to_asterisks(pvalue_dict[\"patient\"], 1))\n",
    "                          }) for ((x, x1), pvalue_dict) in pvalues_list]\n",
    "    return pvalues_list\n",
    "\n",
    "def get_pvalues(dfm, col, x_val=\"dataset\", bf_correction=False):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfm : pandas DataFrame \n",
    "        with columns \"dataset\" and \"speaker\" as either assistant/patient. values in the \"dataset\" column will be paired.\n",
    "    col : str\n",
    "        name of column in dfm that pvalue should be calculated from\n",
    "    x_val : str\n",
    "        name of the column in dfm that is the x axis grouping\n",
    "    bf_correction : bool\n",
    "        whether or not bonferroni correction is applied\n",
    "    '''\n",
    "    x_values = dfm[x_val].unique()\n",
    "    pvalues_list = []\n",
    "    done = []\n",
    "    for x in x_values:\n",
    "        for x1 in x_values:\n",
    "            if x != x1 and x1 not in done:\n",
    "                stat, pvalue = scipy.stats.ttest_ind(\n",
    "                    dfm[dfm[x_val] == x][col],\n",
    "                    dfm[dfm[x_val] == x1][col]\n",
    "                )\n",
    "                pvalues_list.append(((x, x1), pvalue))\n",
    "            done.append(x)\n",
    "\n",
    "    #add asterisks for significance\n",
    "    if bf_correction:\n",
    "        corr_val = len(done)\n",
    "        pvalues_list = [((x, x1), pvalue, convert_pvalue_to_asterisks(pvalue, corr_val)) for ((x, x1), pvalue) in pvalues_list]\n",
    "    else:\n",
    "        pvalues_list = [((x, x1), pvalue, convert_pvalue_to_asterisks(pvalue, 1)) for ((x, x1), pvalue) in pvalues_list]\n",
    "    return pvalues_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph Average Message Length per Role Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### average message length per role utterance\n",
    "g = sns.catplot(\n",
    "    data=labeled_dfm, kind=\"bar\",\n",
    "    x=\"dataset\", y=\"avg_msg_len\", hue=\"speaker\",\n",
    "    errorbar=\"sd\", palette=\"dark\", color=bar_col, alpha=.6, height=6\n",
    ")\n",
    "\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Transcript Creation Method\", \"Avg Message Length (characters)\")\n",
    "g.legend.set_title(\"Role\")\n",
    "\n",
    "\n",
    "g = sns.swarmplot(x=\"dataset\", y=\"avg_msg_len\", hue='speaker', palette=\"dark:black\", alpha=.5, dodge=True,data=labeled_dfm)\n",
    "\n",
    "plt.title(\"Average Message Length per Role Utterance\")\n",
    "plt.legend([],[], frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### average message length per role utterance\n",
    "g = sns.catplot(\n",
    "    data=labeled_dfm, kind=\"bar\",\n",
    "    x=\"speaker\", y=\"avg_msg_len\", hue=\"dataset\",\n",
    "    errorbar=\"sd\", \n",
    "    # color=\"bar_col\",\n",
    "    palette=\"dark\", \n",
    "    alpha=.6, height=6\n",
    ")\n",
    "\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Role\", \"Avg Message Length (characters)\")\n",
    "g.legend.set_title(\"Transcript Creation Method\")\n",
    "\n",
    "g = sns.swarmplot(x=\"speaker\", y=\"avg_msg_len\", hue='dataset', palette=\"dark:black\", alpha=.5, dodge=True,data=labeled_dfm)\n",
    "\n",
    "plt.title(\"Average Message Length per Role Utterance\")\n",
    "plt.legend([],[], frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_msg_len_pvalues = get_dataset_comparison_pvalues(labeled_dfm, \"avg_msg_len\")\n",
    "avg_msg_len_pvalues\n",
    "#do the corrections for multiple comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_msg_len_pvalues_bfcorr = get_dataset_comparison_pvalues(labeled_dfm, \"avg_msg_len\", bf_correction=True) \n",
    "#bonferroni correction: divide by the number of tests (in this case, transcript pairs)\n",
    "avg_msg_len_pvalues_bfcorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conversational rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### conversational rounds\n",
    "g = sns.barplot(\n",
    "    data=labeled_df, \n",
    "    x=\"dataset\", y=\"convo_rounds\",\n",
    "    errorbar=\"sd\", alpha=.6\n",
    ")\n",
    "# g.set_axis_labels(\"Transcript Creation Method\", \"Conversational Rounds\")\n",
    "g = sns.swarmplot(x=\"dataset\", y=\"convo_rounds\",color=\"black\", alpha=.5, dodge=True,data=labeled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pvalues(labeled_df, \"convo_rounds\", \"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at distinct-1 comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dfm1 = pd.melt(labeled_df, \n",
    "                      id_vars=\"dataset\", #data to keep as the identifier\n",
    "                      value_vars=[\"asst_distinct1\", \"user_distinct1\"], #columns to \"unpivot\" or \"melt\" together\n",
    "                      var_name=\"speaker\", #name of new unpivoted col\n",
    "                      value_name=\"distinct-1\") #name of the new col value\n",
    "labeled_dfm1 = labeled_dfm1.replace(\"asst_distinct1\", \"assistant\")\n",
    "labeled_dfm1 = labeled_dfm1.replace(\"user_distinct1\", \"patient\")\n",
    "labeled_dfm1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=labeled_dfm1, kind=\"bar\",\n",
    "    x=\"dataset\", y=\"distinct-1\", hue=\"speaker\",\n",
    "    errorbar=\"sd\", palette=\"dark\", alpha=.6, height=6\n",
    ")\n",
    "\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Transcript Creation Method\", \"Distinct-1 Score\")\n",
    "g.legend.set_title(\"Role\")\n",
    "\n",
    "\n",
    "g = sns.swarmplot(x=\"dataset\", y=\"distinct-1\", hue='speaker', palette=\"dark:black\", alpha=.5, dodge=True,data=labeled_dfm1)\n",
    "\n",
    "plt.title(\"Distinct-1 Score per Role\")\n",
    "plt.legend([],[], frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### average message length per role utterance\n",
    "g = sns.catplot(\n",
    "    data=labeled_dfm1, kind=\"bar\",\n",
    "    x=\"speaker\", y=\"distinct-1\", hue=\"dataset\",\n",
    "    errorbar=\"sd\", \n",
    "    # color=\"bar_col\",\n",
    "    palette=\"dark\", \n",
    "    alpha=.6, height=6\n",
    ")\n",
    "\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Role\", \"Distinct-1 Score\")\n",
    "sns.move_legend(g, \"upper left\", bbox_to_anchor=(0.8, 0.5))\n",
    "g.legend.set_title(\"Transcript Creation Method\")\n",
    "\n",
    "g = sns.swarmplot(x=\"speaker\", y=\"distinct-1\", hue='dataset', palette=\"dark:black\", alpha=.5, dodge=True,data=labeled_dfm1)\n",
    "\n",
    "plt.title(\"Average Message Length per Role Utterance\")\n",
    "plt.legend([],[], frameon=False,bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset_comparison_pvalues(labeled_dfm1, \"distinct-1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
