{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5add52-098f-4dcf-8a51-8b47da545321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4bebf2c-428a-481c-980f-d8076c9e9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Modify this to match the name of the csv file of the patients you generated\n",
    "file_path = \"llm_patients_042425.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7390c26-2d77-41af-a12e-5531fe5ae96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Hard-coded Demographic Distributions\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\" This data is the same as the demographic distributions from the llm_patient_creator.ipynb file. \"\"\"\n",
    "# split distributions to a .py file?\n",
    "\n",
    "# Age distribution (averaged for men & women).\n",
    "AGE_WEIGHTS_MEN = np.array([0.2979, 0.1916, 0.1557, 0.2266, 0.1282])\n",
    "AGE_WEIGHTS_WOMEN = np.array([0.2861, 0.1683, 0.1483, 0.2343, 0.1630])\n",
    "\n",
    "# Relationship status percentages by age group: (18-25, 26-35, 36-45, 46-60, 60+)\n",
    "RELATIONSHIP_STATUS_DISTRIBUTIONS = [ # Single, Long-term relationship, Married/Common-Law, Divorced/Separated, Widowded\n",
    "    [69.26, 23.09,  7.40,  0.25,  0.00],  # 18-25\n",
    "    [31.73, 10.58, 55.35,  2.15,  0.20],  # 26-35\n",
    "    [10.88,  3.63, 76.00,  8.15,  1.35],  # 36-45\n",
    "    [ 9.00,  3.00, 68.33, 15.33,  8.17],  # 46-60\n",
    "    [11.25,  3.75, 54.17,  8.83, 33.50],  # 60+\n",
    "]\n",
    "\n",
    "# Probabilities of being \"in a family\" & \"with children,\" by 4 age bins (0-24, 25-34, 35-44, 45-54).\n",
    "IN_FAMILY = [\n",
    "    [0.0756, 0.0701],  # female, male (0-24)\n",
    "    [0.5102, 0.4765],  # female, male (25-34)\n",
    "    [0.8340, 0.8178],  # female, male (35-44)\n",
    "    [0.8426, 0.8557],  # female, male (45+)\n",
    "]\n",
    "WITH_CHILDREN = [\n",
    "    [0.6997, 0.3003],  # yes, no  (0-24)\n",
    "    [0.5193, 0.4807],  # yes, no  (25-34)\n",
    "    [0.2056, 0.7944],  # yes, no  (35-44)\n",
    "    [0.1771, 0.8229],  # yes, no  (45+)\n",
    "]\n",
    "\n",
    "SEX_WEIGHTS = [0.51, 0.49]\n",
    "SEX_LABELS = [\"Male\", \"Female\"]\n",
    "\n",
    "AGE_ORDER = [\"18-25\", \"26-35\", \"36-45\", \"46-60\", \"60+\"]\n",
    "\n",
    "RELATIONSHIP_ORDER = [\"Single\", \"Long-term relationship\",\n",
    "                          \"Married/Common-Law\", \"Divorced/Separated\", \"Widowded\"]\n",
    "\n",
    "\n",
    "DISABILITY_CATEGORIES = [\n",
    "        \"Mental-health related\", \"Pain-related\", \"Seeing\",\n",
    "        \"Learning\", \"Memory\", \"Mobility\", \"Flexibility\",\n",
    "        \"Hearing\", \"Dexterity\", \"Developmental\"\n",
    "    ]\n",
    "\n",
    "ETHNICITY_PROBABILITY = [0.0243, 0.0334, 0.4313, 0.2328, 0.1417, 0.0763, 0.0198, 0.0158, 0.0251]\n",
    "\n",
    "ETHNICITY_ORDER = [\"Indigenous\", \"Middle Eastern\", \"European\", \n",
    "                   \"East Asian\", \"South Asian\", \"Southeast Asian\", \n",
    "                   \"Latin American\", \"African\", \"Other\"]\n",
    "\n",
    "ETHNICITY_CATEGORIES = {\n",
    "    \"Indigenous\": [\n",
    "        \"Indigenous\"\n",
    "    ],\n",
    "    \"Middle Eastern\": [\"Persian - Iran\", \"Hebrew - Israel\"],\n",
    "    \"European\": [\n",
    "        \"Czech - Czech Republic\", \"Danish - Denmark\", \"German - Austria\", \"German - Switzerland\", \"German - Germany\",\n",
    "        \"Greek - Greece\", \"English - Canada\", \"English - United States\", \"English - United Kingdom\", \"English - Ireland\",\n",
    "        \"Spanish - Spain\", \"Finnish - Finland\", \"French - Canada\", \"French - Switzerland\", \"French - France\",\n",
    "        \"Croatian - Croatia\", \"Hungarian - Hungary\", \"Armenian - Armenia\", \"Italian - Italy\", \"Dutch - Belgium\",\n",
    "        \"Dutch - Netherlands\", \"Norwegian - Norway\", \"Polish - Poland\", \"Portuguese - Portugal\", \"Romanian - Romania\",\n",
    "        \"Russian - Russia\", \"Slovak - Slovakia\", \"Swedish - Sweden\", \"Ukrainian - Ukraine\"\n",
    "    ],\n",
    "    \"East Asian\": [\n",
    "        \"Japanese - Japan\", \"Korean - South Korea\",\n",
    "        \"Chinese - China\", \"Chinese - Taiwan\"\n",
    "    ],\n",
    "    \"South Asian\": [\n",
    "        \"English - India\", \"Hindi - India\", \"Nepali - Nepal\",\n",
    "        \"Tamil - India\", \"Bengali - Bangladesh\", \"English - Bangladesh\",\n",
    "        \"Georgian - Georgia\", \"Azerbaijani - Azerbaijan\"\n",
    "    ],\n",
    "    \"Southeast Asian\": [\n",
    "        \"English - Malaysia\", \"English - Philippines\", \"Filipino - Philippines\",\n",
    "        \"Indonesian - Indonesia\", \"Thai - Thailand\", \"Tagalog - Philippines\",\n",
    "        \"Vietnamese - Vietnam\"\n",
    "    ],\n",
    "    \"Latin American\": [\n",
    "        \"Spanish - Argentina\", \"Spanish - Chile\", \"Spanish - Colombia\",\n",
    "        \"Spanish - Mexico\", \"Portuguese - Brazil\"\n",
    "    ],\n",
    "    \"African\": [\"Zulu - South Africa\"],\n",
    "    \"Other\": [\"English - Australia\", \"English - New Zealand\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a877dc9-002a-44d4-a018-7d62e67d365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Statistical Tests\n",
    "# =============================================================================\n",
    "\n",
    "def binomial_p_value(n, p, observed_count, tail='right'):\n",
    "    \"\"\"\n",
    "    Compute the p-value for a binomial distribution using a normal approximation.\n",
    "    \"\"\"\n",
    "    expected = n * p\n",
    "    print(\"expected:\", expected)\n",
    "    sigma = np.sqrt(n * p * (1 - p))\n",
    "    print(\"sigma:\", sigma)\n",
    "    print(\"obs:\", observed_count)\n",
    "    z_score = (observed_count - expected) / sigma\n",
    "    print(\"z:\", z_score)\n",
    "    \n",
    "    if observed_count > expected:\n",
    "        print(\"right\")\n",
    "        print(\"observed:\", observed_count)\n",
    "        print(\"expected:\", expected)\n",
    "        p_value = 1 - stats.norm.cdf(z_score)\n",
    "    else:  # left tail\n",
    "        print(\"left\")\n",
    "        p_value = stats.norm.cdf(z_score)\n",
    "    \n",
    "    return z_score, p_value\n",
    "\n",
    "def multinomial_p_value(observed_counts, expected_probs):\n",
    "    \"\"\"\n",
    "    Compute the p-value for a multinomial distribution using the chi-square test.\n",
    "    \"\"\"\n",
    "    total_n = sum(observed_counts)\n",
    "    expected_counts = np.array(expected_probs) * total_n\n",
    "    print(\"expected:\", expected_counts)\n",
    "    print(\"observed:\", observed_counts)\n",
    "    chi_square_stat, p_value = stats.chisquare(f_obs=observed_counts, f_exp=expected_counts)\n",
    "    # chi_square_stat = 0\n",
    "    # p_value = 0\n",
    "    return chi_square_stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b01964f-fb26-4085-92a1-16096e7cc2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Analysis and Plotting\n",
    "# =============================================================================\n",
    "\n",
    "def analyze(df, column, expected_probs, title, chart_type='bar', order=None):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis for a given categorical column.\n",
    "    \"\"\"\n",
    "\n",
    "    # If an order is provided, reindex the observed counts accordingly\n",
    "    if order is not None:\n",
    "        observed_counts = df[column].value_counts().reindex(order, fill_value=0)\n",
    "    else:\n",
    "        observed_counts = df[column].value_counts().sort_index()\n",
    "        \n",
    "    print(\"counts:\", observed_counts)\n",
    "    print(type(observed_counts))\n",
    "    observed_probs = observed_counts / observed_counts.sum()\n",
    "    print(\"probs:\", observed_probs)\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    expected_probs = np.array(expected_probs) / np.sum(expected_probs)\n",
    "    \n",
    "    if len(expected_probs) == 2:\n",
    "        # Binomial test\n",
    "        print(observed_counts.sum(), expected_probs[1], observed_counts.iloc[1])\n",
    "        z_score, p_value = binomial_p_value(observed_counts.sum(), expected_probs[1], observed_counts.iloc[1])\n",
    "        print(f\"{title} - Z-score: {z_score:.2f}, P-value: {p_value:.4f}\")\n",
    "    else:\n",
    "        # Multinomial test\n",
    "        chi_square_stat, p_value = multinomial_p_value(observed_counts.values, expected_probs)\n",
    "        print(f\"{title} - Chi-Square: {chi_square_stat:.2f}, P-value: {p_value:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ffcbe3c-e67a-4885-a6c7-411c083d689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "def categorize_ethnicity(ethnicity):\n",
    "    for category, values in ETHNICITY_CATEGORIES.items():\n",
    "         if ethnicity in values:\n",
    "            return category\n",
    "    return \"Uncategorized\"\n",
    "\n",
    "\n",
    "def compute_expected_disability(row: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Hard-coded probability of being disabled by (sex, age),\n",
    "    then distribute among 10 disability types.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hard-coded distribution across 10 types\n",
    "    s = 38.6 + 61.8 + 27.4 + 20.7 + 18.2 + 39.2 + 40.3 + 20.7 + 18.4 + 5.7\n",
    "    DISABILITY_PROBABILITY_TYPES_NORM = [\n",
    "        38.6/s, 61.8/s, 27.4/s, 20.7/s, 18.2/s,\n",
    "        39.2/s, 40.3/s, 20.7/s, 18.4/s, 5.7/s\n",
    "    ]\n",
    "    expected_values = {\n",
    "        cat: p_type\n",
    "        for cat, p_type in zip(DISABILITY_CATEGORIES, DISABILITY_PROBABILITY_TYPES_NORM)\n",
    "    }\n",
    "    return pd.Series(expected_values)\n",
    "\n",
    "def categorize_age(age):\n",
    "    \"\"\"\n",
    "    Categorize age into groups:\n",
    "    18-25, 26-35, 36-45, 46-60, 60+.\n",
    "    \"\"\"\n",
    "    if age < 18:\n",
    "        return \"Under 18\"\n",
    "    elif 18 <= age <= 25:\n",
    "        return \"18-25\"\n",
    "    elif 26 <= age <= 35:\n",
    "        return \"26-35\"\n",
    "    elif 36 <= age <= 45:\n",
    "        return \"36-45\"\n",
    "    elif 46 <= age <= 60:\n",
    "        return \"46-60\"\n",
    "    else:\n",
    "        return \"60+\"\n",
    "\n",
    "def compute_expected_age_probs(df):\n",
    "    \"\"\"\n",
    "    Compute overall expected age probabilities using provided weights and the sex distribution in df.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate proportion of males and females in the dataset\n",
    "    sex_counts = df['Sex'].value_counts(normalize=True)\n",
    "    prop_male = sex_counts.get(\"Male\", 0)\n",
    "    prop_female = sex_counts.get(\"Female\", 0)\n",
    "    \n",
    "    overall_expected = prop_male * AGE_WEIGHTS_MEN + prop_female * AGE_WEIGHTS_WOMEN\n",
    "    print(\"Expected age probabilities:\", overall_expected)\n",
    "    return overall_expected, AGE_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5a7b01-92d3-4518-aa28-7907e8ca94d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Statistical Tests and Graphs ===\n",
      "\n",
      "counts: Sex\n",
      "Female    520\n",
      "Male      480\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n",
      "probs: Sex\n",
      "Female    0.52\n",
      "Male      0.48\n",
      "Name: count, dtype: float64\n",
      "1000 0.49 480\n",
      "expected: 490.0\n",
      "sigma: 15.8082257068907\n",
      "obs: 480\n",
      "z: -0.6325820611000681\n",
      "left\n",
      "Sex Distribution - Z-score: -0.63, P-value: 0.2635\n",
      "Expected age probabilities: [0.291764 0.179484 0.151852 0.230604 0.146296]\n",
      "counts: Age_Category\n",
      "18-25    276\n",
      "26-35    188\n",
      "36-45    158\n",
      "46-60    243\n",
      "60+      135\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n",
      "probs: Age_Category\n",
      "18-25    0.276\n",
      "26-35    0.188\n",
      "36-45    0.158\n",
      "46-60    0.243\n",
      "60+      0.135\n",
      "Name: count, dtype: float64\n",
      "expected: [291.764 179.484 151.852 230.604 146.296]\n",
      "observed: [276 188 158 243 135]\n",
      "Age Distribution - Chi-Square: 3.04, P-value: 0.5506\n",
      "counts: Ethnicity_Category\n",
      "Indigenous          28\n",
      "Middle Eastern      40\n",
      "European           440\n",
      "East Asian         204\n",
      "South Asian        142\n",
      "Southeast Asian     86\n",
      "Latin American      15\n",
      "African             15\n",
      "Other               30\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n",
      "probs: Ethnicity_Category\n",
      "Indigenous         0.028\n",
      "Middle Eastern     0.040\n",
      "European           0.440\n",
      "East Asian         0.204\n",
      "South Asian        0.142\n",
      "Southeast Asian    0.086\n",
      "Latin American     0.015\n",
      "African            0.015\n",
      "Other              0.030\n",
      "Name: count, dtype: float64\n",
      "expected: [ 24.28785607  33.38330835 431.08445777 232.68365817 141.62918541\n",
      "  76.26186907  19.79010495  15.79210395  25.08745627]\n",
      "observed: [ 28  40 440 204 142  86  15  15  30]\n",
      "Ethnicity Distribution - Chi-Square: 9.00, P-value: 0.3419\n",
      "total obs: 498\n",
      "counts: Mental-health related    62\n",
      "Pain-related             83\n",
      "Seeing                   56\n",
      "Learning                 40\n",
      "Memory                   37\n",
      "Mobility                 70\n",
      "Flexibility              63\n",
      "Hearing                  43\n",
      "Dexterity                32\n",
      "Developmental            12\n",
      "dtype: int64\n",
      "expected: [ 66.05773196 105.76082474  46.89072165  35.42474227  31.14639175\n",
      "  67.08453608  68.96701031  35.42474227  31.48865979   9.75463918]\n",
      "observed: [62 83 56 40 37 70 63 43 32 12]\n",
      "Disability Breakdown - Chi-Square: 11.40, P-value: 0.2495\n",
      "\n",
      "=== Relationship Status Goodness-of-Fit by Age Group ===\n",
      "\n",
      "expected: [176.613   58.8795  18.87     0.6375   0.    ]\n",
      "observed: [202  53   0   0   0]\n",
      "  18-25 → χ² = nan, p-value = nan\n",
      "expected: [21.2591  7.0886 37.0845  1.4405  0.134 ]\n",
      "observed: [45 22  0  0  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ninc-user/miniconda3/envs/patient_creator/lib/python3.10/site-packages/scipy/stats/_stats_py.py:7335: RuntimeWarning: invalid value encountered in divide\n",
      "  terms = (f_obs - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For each axis slice, the sum of the observed frequencies must agree with the sum of the expected frequencies to a relative tolerance of 1.4901161193847656e-08, but the percent differences are:\n9.999999999992602e-05",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2523166/2253742944.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m                               \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAGE_ORDER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                               columns=RELATIONSHIP_ORDER)\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2523166/2253742944.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mobs_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts_rel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# RELATIONSHIP_STATUS_DISTRIBUTIONS[idx] should be a list of percentages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mexp_probs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRELATIONSHIP_STATUS_DISTRIBUTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultinomial_p_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{age:>7} → χ² = {chi2:.2f}, p-value = {pval:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# =============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2523166/4294897252.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(observed_counts, expected_probs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtotal_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserved_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mexpected_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_probs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"expected:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"observed:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserved_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mchi_square_stat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchisquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_obs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_exp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m# chi_square_stat = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# p_value = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mchi_square_stat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/patient_creator/lib/python3.10/site-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(f_obs, f_exp, ddof, axis, sum_check)\u001b[0m\n\u001b[1;32m   7502\u001b[0m     \u001b[0mPower_divergenceResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3.5\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m9.25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.62338763\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.09949846\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7504\u001b[0m     \u001b[0mFor\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdetailed\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mhypothesis_chisquare\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7505\u001b[0m     \"\"\"  # noqa: E501\n\u001b[0;32m-> 7506\u001b[0;31m     return _power_divergence(f_obs, f_exp=f_exp, ddof=ddof, axis=axis,\n\u001b[0m\u001b[1;32m   7507\u001b[0m                              lambda_=\"pearson\", sum_check=sum_check)\n",
      "\u001b[0;32m~/miniconda3/envs/patient_creator/lib/python3.10/site-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(f_obs, f_exp, ddof, axis, lambda_, sum_check)\u001b[0m\n\u001b[1;32m   7318\u001b[0m                        \u001b[0;34mf\"frequencies must agree with the sum of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7319\u001b[0m                        \u001b[0;34mf\"expected frequencies to a relative tolerance \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7320\u001b[0m                        \u001b[0;34mf\"of {rtol}, but the percent differences are:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7321\u001b[0m                        f\"{relative_diff}\")\n\u001b[0;32m-> 7322\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7324\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7325\u001b[0m         \u001b[0;31m# Ignore 'invalid' errors so the edge case of a data set with length 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: For each axis slice, the sum of the observed frequencies must agree with the sum of the expected frequencies to a relative tolerance of 1.4901161193847656e-08, but the percent differences are:\n9.999999999992602e-05"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    df = pd.read_csv(file_path, delimiter = \"|\")\n",
    "    \n",
    "    print(\"\\n=== Statistical Tests and Graphs ===\\n\")\n",
    "    \n",
    "    # == Sex Analysis ==\n",
    "    analyze(df, 'Sex', SEX_WEIGHTS, \"Sex Distribution\")\n",
    "\n",
    "    # == Age Analysis== \n",
    "    df['Age_Category'] = df['Age'].apply(categorize_age)\n",
    "    expected_age_probs, age_groups = compute_expected_age_probs(df)\n",
    "    # Ensure the Age_Category is ordered correctly\n",
    "    df['Age_Category'] = pd.Categorical(df['Age_Category'], categories=age_groups, ordered=True)\n",
    "    analyze(df, 'Age_Category', expected_age_probs, \"Age Distribution\", order=age_groups)\n",
    "\n",
    "    # == Ethnicity Analysis ==\n",
    "    df[\"Ethnicity_Category\"] = df[\"Ethnicity\"].apply(categorize_ethnicity)\n",
    "    analyze(df, 'Ethnicity_Category', ETHNICITY_PROBABILITY, \"Ethnicity Distribution\", order=ETHNICITY_ORDER)\n",
    "    \n",
    "    # == Disability Analysis ==\n",
    "    df_yes_disability = df[df[\"Disabled\"] == \"Yes\"]\n",
    "    # 1) Parse and flatten the disability type lists\n",
    "    all_types = []\n",
    "\n",
    "    for val in df_yes_disability[\"Disability\"]:\n",
    "        if isinstance(val, str):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(val)  # safely converts \"['mobility', 'vision']\" → list\n",
    "                if isinstance(parsed, list):\n",
    "                    all_types.extend(parsed)\n",
    "            except:\n",
    "                continue\n",
    "        elif isinstance(val, list):\n",
    "            all_types.extend(val)\n",
    "            \n",
    "    # 2) Count frequencies\n",
    "    type_counts = Counter(all_types)\n",
    "    # print(\"type counts:\", type_counts)\n",
    "    # total_obs = df_yes_disability.shape[0]\n",
    "    total_obs = sum(type_counts.values())\n",
    "    print(\"total obs:\", total_obs)\n",
    "\n",
    "    # 3) Build observed distribution dict\n",
    "    observed_disability = {\n",
    "        cat: type_counts.get(cat, 0) / total_obs\n",
    "        for cat in DISABILITY_CATEGORIES\n",
    "    }\n",
    "\n",
    "    expected_disability_df = df_yes_disability.apply(compute_expected_disability, axis=1)\n",
    "    expected_disability_avg = expected_disability_df.mean().to_dict()\n",
    "\n",
    "    categories = list(expected_disability_avg.keys())\n",
    "\n",
    "    observed_counts = pd.Series(type_counts)\n",
    "    observed_counts = observed_counts.reindex(DISABILITY_CATEGORIES)\n",
    "    print(\"counts:\" , observed_counts)\n",
    "    expected_counts = np.array([expected_disability_avg[cat] for cat in categories])\n",
    "\n",
    "    expected_probs = np.array(expected_counts) / np.sum(expected_counts)\n",
    "\n",
    "    # observed_counts = df_yes_disability['Disability'].value_counts().reindex(order, fill_value=0)\n",
    "\n",
    "    title=\"Disability Breakdown\"\n",
    "    chi_square_stat, p_value = multinomial_p_value(observed_counts.values, expected_probs)\n",
    "    print(f\"{title} - Chi-Square: {chi_square_stat:.2f}, P-value: {p_value:.4f}\")\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={\n",
    "        \"Relationship Status\": \"Relationship_Status\",\n",
    "        \"Children\": \"Has_Children\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Convert \"Has_Children\" to yes/no\n",
    "    df[\"Has_Children\"] = df[\"Has_Children\"].apply(\n",
    "        lambda x: \"Yes\" if x != 0 else \"No\"\n",
    "    )\n",
    "    \n",
    "    # # =============================================================================\n",
    "    # # == Goodness-of-Fit: Relationship Status by Age Group ==\n",
    "    # # =============================================================================\n",
    "    # print(\"\\n=== Relationship Status Goodness-of-Fit by Age Group ===\\n\")\n",
    "\n",
    "    # # 1) Build the raw counts table\n",
    "    # #    Make sure your df has the same age‐category ordering you used above:\n",
    "    # counts_rel_df = (\n",
    "    #     pd.crosstab(df[\"Age_Category\"],  # or df[\"Age_Group\"] if that’s your column\n",
    "    #                 df[\"Relationship_Status\"])\n",
    "    #       .reindex(index=age_groups,    # age_groups from compute_expected_age_probs\n",
    "    #                columns=RELATIONSHIP_ORDER,   # [\"Single\", \"Long-term relationship\", …]\n",
    "    #                fill_value=0)\n",
    "    # )\n",
    "\n",
    "    # # 2) Loop over each age bin and run the χ² goodness-of-fit\n",
    "    # for idx, age in enumerate(age_groups):\n",
    "    #     obs_counts = counts_rel_df.loc[age].values\n",
    "    #     # RELATIONSHIP_STATUS_DISTRIBUTIONS[idx] should be a list of percentages\n",
    "    #     exp_probs  = np.array(RELATIONSHIP_STATUS_DISTRIBUTIONS[idx]) / 100.0\n",
    "\n",
    "    #     chi2, pval = multinomial_p_value(obs_counts, exp_probs)\n",
    "    #     print(f\"{age:>7} → χ² = {chi2:.2f}, p-value = {pval:.4f}\")\n",
    "\n",
    "    # # =============================================================================\n",
    "    # # == Has Children by Age & Sex (binomial Z-tests) ==\n",
    "    # # =============================================================================\n",
    "    # # rebuild age groups if necessary\n",
    "    # df['Age_Category'] = pd.Categorical(df['Age_Category'],\n",
    "    #                                     categories=AGE_ORDER,\n",
    "    #                                     ordered=True)\n",
    "\n",
    "    # # 1) Raw yes/no counts\n",
    "    # counts_ch = (\n",
    "    #     df.groupby([\"Age_Category\", \"Sex\"])[\"Has_Children\"]\n",
    "    #       .value_counts()\n",
    "    #       .unstack(fill_value=0)\n",
    "    # )\n",
    "\n",
    "    # print(\"\\n== Has Children by Age & Sex ==\")\n",
    "    # for idx, age in enumerate(AGE_ORDER):\n",
    "    #     for sex_idx, sex in enumerate([\"Female\", \"Male\"]):\n",
    "    #         row = counts_ch.loc[(age, sex)]\n",
    "    #         n_total = row.sum()\n",
    "    #         yes_obs = row.get(\"Yes\", 0)\n",
    "    #         # expected probability = IN_FAMILY[idx][sex_idx] * WITH_CHILDREN[idx][1]\n",
    "    #         exp_prob = IN_FAMILY[idx][sex_idx] * WITH_CHILDREN[idx][1]\n",
    "    #         z, pval = binomial_p_value(n_total, exp_prob, yes_obs)\n",
    "    #         print(f\"{age:>7}, {sex:<6} → Z = {z:.2f}, p = {pval:.4f}\")\n",
    "\n",
    "\n",
    "    # 1) Compute observed fractions using crosstab (normalized by row)\n",
    "    observed_rel_df = pd.crosstab(df[\"Age_Group\"],\n",
    "                                  df[\"Relationship_Status\"],\n",
    "                                  normalize=\"index\")\n",
    "    observed_rel_df = observed_rel_df.reindex(index=AGE_ORDER,\n",
    "                                              columns=RELATIONSHIP_ORDER)\n",
    "\n",
    "    # 2) Build expected distribution from hard-coded RELATIONSHIP_STATUS_DISTRIBUTIONS\n",
    "    columns_for_rel = [\"Single\", \"Long-term Relationship\", \"Married/Common-law\",\n",
    "                       \"Divorced/Separated\", \"Widowed\"]\n",
    "    rel_df = pd.DataFrame(RELATIONSHIP_STATUS_DISTRIBUTIONS,\n",
    "                          index=AGE_ORDER,\n",
    "                          columns=columns_for_rel)\n",
    "\n",
    "    # 3) Rename columns to match observed naming\n",
    "    col_map = {\n",
    "        \"Long-term Relationship\": \"Long-term relationship\",\n",
    "        \"Married/Common-law\": \"Married/Common-Law\",\n",
    "        \"Widowed\": \"Widowded\"\n",
    "    }\n",
    "    rel_df.rename(columns=col_map, inplace=True)\n",
    "    expected_rel_df = rel_df.reindex(index=AGE_ORDER, columns=RELATIONSHIP_ORDER)\n",
    "\n",
    "    # 4) Convert from percentages (0-100) to fractions (0-1)\n",
    "    observed_matrix = observed_rel_df.values\n",
    "    expected_matrix = expected_rel_df.values / 100.0\n",
    "\n",
    "    # Convert the matrix form back into DataFrame (for the split-diagonal function)\n",
    "    obs_df_rel = pd.DataFrame(observed_matrix,\n",
    "                              index=AGE_ORDER,\n",
    "                              columns=RELATIONSHIP_ORDER)\n",
    "    exp_df_rel = pd.DataFrame(expected_matrix,\n",
    "                              index=AGE_ORDER,\n",
    "                              columns=RELATIONSHIP_ORDER)\n",
    "\n",
    "    print(\"obs: \", obs_df_rel)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f681c-8299-4827-96ee-57a5712e582b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (patient_creator)",
   "language": "python",
   "name": "patient_creator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
